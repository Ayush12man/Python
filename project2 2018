Extream
df['processor_gnrtn'] = df['processor_gnrtn'].replace('Not Available', np.nan)
print(df.head(2))
#Replacing  the 'Not available' observations as null values as XGBoost deals with missing obsn.
# Define the mapping for each variable
ram_mapping = {'4 GB': 0, '8 GB': 1, '16 GB': 2, '32 GB': 3}
ssd_mapping = {'0 GB': 0, '128 GB': 1, '256 GB': 2, '512 GB': 3, '1024 GB': 4, '2048 GB': 5, '3072 GB': 6}
hdd_mapping = {'0 GB': 0, '512 GB': 1, '1024 GB': 2, '2048 GB': 3}
graphic_card_gb_mapping = {'0 GB': 0, '2 GB': 1, '4 GB': 2, '6 GB': 3, '8 GB': 4}
rating_mapping = {'1 star': 0, '2 stars': 1, '3 stars': 2, '4 stars': 3, '5 stars': 4}
warr= {'No warranty': 0, '1 year': 1, '2 years': 2, '3 years': 3}
# Apply the mapping to the DataFrame
df['ram_gb'] = df['ram_gb'].map(ram_mapping)
df['ssd'] = df['ssd'].map(ssd_mapping)
df['hdd'] = df['hdd'].map(hdd_mapping)
df['graphic_card_gb'] = df['graphic_card_gb'].map(graphic_card_gb_mapping)
df['rating'] = df['rating'].map(rating_mapping)
df['warranty'] = df['warranty'].map(warr)
# Display the updated DataFrame
print(df.head())
df_dummies = pd.get_dummies(df, drop_first=True)
df_dummies.head(2)
from sklearn.model_selection import train_test_split
X = df_dummies.drop('Price', axis=1)  # Features
y = df_dummies['Price']  # Target variable
Data Preprocessing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print("X_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)
print("y_train shape:", y_train.shape)
print("y_test shape:", y_test.shape)
model with grid
import xgboost as xgb
from sklearn.model_selection import GridSearchCV
# Define the XGBoost regressor
xgboost_regressor = xgb.XGBRegressor()

# Define the hyperparameter grid
param_grid = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 5, 7],
    'subsample': [0.8, 1.0],
    'colsample_bytree': [0.8, 1.0]
}

# Perform grid search with cross-validation
grid_search = GridSearchCV(estimator=xgboost_regressor, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1, verbose=2)
grid_search.fit(X, y)

# Display the best parameters and the best score
print("Best parameters found: ", grid_search.best_params_)
print("Best score: ", grid_search.best_score_)
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score
import pandas as pd

# Get the best parameters from the grid search
best_params = grid_search.best_params_

# Fit the XGBoost model with the best parameters
best_xgboost_regressor = xgb.XGBRegressor(**best_params)
best_xgboost_regressor.fit(X_train, y_train)

# Predict on the test set
y_pred = best_xgboost_regressor.predict(X_test)

# Calculate the R^2 value
r2 = r2_score(y_test, y_pred)
print("R^2 value:", r2)
from sklearn.metrics import mean_absolute_error
mae = mean_absolute_error(y_test, y_pred)
print("Mean Absolute Error (MAE):", mae)
from sklearn.metrics import mean_squared_error
mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error (MSE):", mse)
rmse = np.sqrt(mse)
print("Root Mean Squared Error (RMSE):", rmse)

without grid
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score
import pandas as pd


# Define the XGBoost regressor with default parameters
xgboost_regressor = xgb.XGBRegressor()

# Fit the XGBoost model
xgboost_regressor.fit(X_train, y_train)

# Predict on the test set
y_pred = xgboost_regressor.predict(X_test)

# Calculate the R^2 value
r2 = r2_score(y_test, y_pred)
print("R^2 value:", r2)

import xgboost as xgb
from sklearn.model_selection import RandomizedSearchCV
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score
# Define the XGBoost regressor
xgboost_regressor = xgb.XGBRegressor()

# Define the hyperparameter grid
param_dist = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 5, 7],
    'subsample': [0.8, 1.0],
    'colsample_bytree': [0.8, 1.0]
}

# Perform randomized search with cross-validation
random_search = RandomizedSearchCV(estimator=xgboost_regressor, param_distributions=param_dist, n_iter=50, cv=5, scoring='neg_mean_squared_error', n_jobs=-1, random_state=42, verbose=2)
random_search.fit(X_train, y_train)

# Get the best parameters from the randomized search
best_params = random_search.best_params_
print("Best parameters found: ", best_params)

# Fit the XGBoost model with the best parameters
best_xgboost_regressor = xgb.XGBRegressor(**best_params)
best_xgboost_regressor.fit(X_train, y_train)

# Predict on the test set
y_pred = best_xgboost_regressor.predict(X_test)

# Calculate the R^2 value
r2 = r2_score(y_test, y_pred)
print("R^2 value:", r2)

