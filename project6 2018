import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.datasets import cifar10
import warnings
warnings.filterwarnings('ignore')
# Load CIFAR-10 dataset
(x_train, y_train), (x_test, y_test) = cifar10.load_data()

print("Training set shape:", x_train.shape)
print("Test set shape:", x_test.shape)
# Class labels
class_names = ['Airplane', 'Automobile', 'Bird', 'Cat', 'Deer',
               'Dog', 'Frog', 'Horse', 'Ship', 'Truck']

# Plot first 10 images with class names
plt.figure(figsize=(10, 2))
for i in range(10):
    plt.subplot(1, 10, i+1)
    plt.imshow(x_train[i])
    plt.title(class_names[int(y_train[i])])
    plt.axis('off')
plt.tight_layout()
plt.show()

x_train = x_train.astype('float32') / 255.0
x_test = x_test.astype('float32') / 255.0
# One-hot encode the labels
y_train = to_categorical(y_train, 10)
y_test = to_categorical(y_test, 10)
model = models.Sequential()

# Convolutional Layer 1
model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))
model.add(layers.MaxPooling2D((2, 2)))

# Convolutional Layer 2
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))

# Convolutional Layer 3
model.add(layers.Conv2D(64, (3, 3), activation='relu'))

# Flatten + Dense Layers
model.add(layers.Flatten())
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(10, activation='softmax'))  # 10 classes
model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

model.summary()

history = model.fit(x_train, y_train, epochs=10,
                    validation_data=(x_test, y_test),
                    batch_size=64)
plt.plot(history.history['accuracy'], label='Train Acc')
plt.plot(history.history['val_accuracy'], label='Test Acc')
plt.title("Accuracy over Epochs")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()
plt.show()

plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Test Loss')
plt.title("Loss over Epochs")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.show()
test_loss, test_acc = model.evaluate(x_test, y_test)
print("Test Accuracy: {:.2f}%".format(test_acc * 100))

222

import tensorflow as tf
from tensorflow import keras
import numpy as np
fashion_mnist=keras.datasets.fashion_mnist
(train_images,train_labels),(test_images,test_labels)=fashion_mnist.load_data()

train_images=train_images/255
test_images=test_images/255

train_images[0].shape

train_images=train_images.reshape(len(train_images),28,28,1)
test_images=test_images.reshape(len(test_images),28,28,1)
train_images.shape
test_images.shape

# Define the model architecture directly (no build_model function)
model = keras.Sequential([
    # First Convolutional Layer
    keras.layers.Conv2D(
        filters=64,                # Number of filters (e.g., 64)
        kernel_size=3,             # Kernel size (3x3)
        activation='relu',
        input_shape=(28, 28, 1)    # Input shape of the image (28x28 grayscale)
    ),

    # Second Convolutional Layer
    keras.layers.Conv2D(
        filters=64,                # Number of filters (e.g., 64)
        kernel_size=3,             # Kernel size (3x3)
        activation='relu'
    ),

    # Flatten the 2D output to 1D for the Dense layers
    keras.layers.Flatten(),

    # Fully Connected Dense Layer
    keras.layers.Dense(
        units=64,                  # Number of neurons in this layer (e.g., 64)
        activation='relu'
    ),

    # Output Layer (10 classes for Fashion MNIST)
    keras.layers.Dense(10, activation='softmax')  # 10 output classes for Fashion MNIST
])

# Compile the model with the Adam optimizer and a fixed learning rate
model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=0.001),  # Fixed learning rate
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# Train the model
history=model.fit(train_images, train_labels , epochs=10, batch_size=32, validation_data=(test_images, test_labels))

# Access the accuracy history
train_accuracy = history.history['accuracy']          # Training accuracy per epoch
val_accuracy = history.history['val_accuracy']       # Validation accuracy per epoch

# Print the training and validation accuracy at the end of the last epoch
print(f"Training Accuracy: {train_accuracy[-1]:.4f}")
print(f"Validation Accuracy: {val_accuracy[-1]:.4f}")

from tensorflow import keras

def build_model(hp):
    model = keras.Sequential([
        # First Convolutional Layer
        keras.layers.Conv2D(
            filters=hp.Int('conv_1_filter', min_value=32, max_value=128, step=16),#no. of filters
            kernel_size=hp.Choice('conv_1_kernel', values=[3, 5]),# dimension of filter matrix
            activation='relu',
            padding='same',  # Added padding layer
            input_shape=(28, 28, 1)
        ),

        # First MaxPooling Layer
        keras.layers.MaxPooling2D(
            pool_size=hp.Choice('pool_1_size', values=[2, 3]),  # Pool size can be 2x2 or 3x3
            strides=hp.Int('pool_1_stride', min_value=1, max_value=2, step=1)  # Stride can be 1 or 2
        ),

        # Second Convolutional Layer
        keras.layers.Conv2D(
            filters=hp.Int('conv_2_filter', min_value=32, max_value=64, step=16),
            kernel_size=hp.Choice('conv_2_kernel', values=[3, 5]),
            padding='same',  # Added padding layer
            activation='relu'
        ),

        # Second MaxPooling Layer
        keras.layers.MaxPooling2D(
            pool_size=hp.Choice('pool_2_size', values=[2, 3]),  # Pool size can be 2x2 or 3x3
            strides=hp.Int('pool_2_stride', min_value=1, max_value=2, step=1)  # Stride can be 1 or 2
        ),

        # Flatten the 3D feature maps to 1D vector
        keras.layers.Flatten(),

        # Dense Layer
        keras.layers.Dense(
            units=hp.Int('dense_1_units', min_value=32, max_value=128, step=16),
            activation='relu'
        ),

        #The Dense layer is a fully connected layer in a neural network, meaning each neuron in this layer
        #is connected to every neuron in the previous layer.
        #It is typically used in the final stages of the network
        #(after convolutional and pooling layers), but it can be used anywhere within
        #a network to integrate learned features from previous layers.

        # Output Layer (10 classes for classification)
        keras.layers.Dense(10, activation='softmax')
    ])

    # Model compilation with Adam optimizer and sparse categorical cross-entropy loss
    model.compile(optimizer=keras.optimizers.Adam(hp.Choice('learning_rate', values=[1e-2, 1e-3])),
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])

    return model

import keras_tuner
from keras_tuner import RandomSearch
from keras_tuner.engine.hyperparameters import HyperParameters

tuner_search=RandomSearch(build_model,
                          objective='val_accuracy',
                          max_trials=5,
                          directory='output',
                          project_name="Mnist Fashion")

tuner_search.search(train_images,train_labels,epochs=3,validation_split=0.1)

model=tuner_search.get_best_models(num_models=1)[0]
model

best_hps = tuner_search.get_best_hyperparameters(num_trials=1)[0]
print("Best Hyperparameters: ", best_hps.values)

model.summary()

